{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain_community langgraph langchain-huggingface transformers torch"
      ],
      "metadata": {
        "id": "tKFcCma5W5Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install unstructured\n",
        "\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader\n"
      ],
      "metadata": {
        "id": "SuhXQUjbbNXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader"
      ],
      "metadata": {
        "id": "JBUDslffb1Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "id": "KfGNWwMtg3Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZnZAQTDWc-U"
      },
      "outputs": [],
      "source": [
        "urls = ['https://langchain-ai.github.io/langgraph/tutorials/introduction/']\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "docs = loader.load()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "Xur9rJsuh1Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(\"Total number of documents: \",len(all_splits))"
      ],
      "metadata": {
        "id": "0jPazq9UfXbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits"
      ],
      "metadata": {
        "id": "lrAVIs6xj_ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Embedding models: https://python.langchain.com/v0.1/docs/integrations/text_embedding/\n",
        "\n",
        "# Let's load the Hugging Face Embedding class.  sentence_transformers\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "vector[:5]"
      ],
      "metadata": {
        "id": "SdyNZaOfkKgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we want to convert all 98 embeddings"
      ],
      "metadata": {
        "id": "uYepkJRpm--O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LBK0787JnYBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i4F_IFK2nJUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!pip install langchain_chroma\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=HuggingFaceEmbeddings())\n",
        "\n",
        "# if you want to store chromadb locally\n",
        "\n",
        "# vectorstore = Chroma.from_documents(\n",
        "#     documents=docs,\n",
        "#     embedding=HuggingFaceEmbeddings(),\n",
        "#     persist_directory=\"./my_chroma_db\"  # Custom directory\n",
        "# )\n",
        "\n",
        "\n",
        "# Loading the Database Later\n",
        "# This reloads the previously stored embeddings so you donâ€™t have to recompute them.\n",
        "\n",
        "# vectorstore = Chroma(\n",
        "#     persist_directory=\"./my_chroma_db\",\n",
        "#     embedding_function=HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "yQ4isj-1m7TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "#model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "model_id = \"tiiuae/falcon-7b\"\n",
        "\n",
        "# text_generation_pipeline = pipeline(\n",
        "#     \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, max_new_tokens=400, device=0)\n",
        "\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    max_new_tokens=200,\n",
        "    device=0,\n",
        "    temperature=0.7,  #  (lower values = more deterministic)\n",
        "    top_k=50,  # Filters out low-probability tokens\n",
        ")\n",
        "\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "w73LezlaoUwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "# If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "# Use three sentences maximum and keep the answer as concise as possible.\n",
        "# Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        " #{context}\n",
        "\n",
        " #Question: {question}\n",
        "\n",
        " #Helpful Answer:\"\"\"\n",
        "# prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        ""
      ],
      "metadata": {
        "id": "pgb8gmTwoVP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n"
      ],
      "metadata": {
        "id": "dQ4ZlisDoVTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define application steps\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vectorstore.similarity_search(state[\"question\"],  k=1)\n",
        "    return {\"context\": retrieved_docs}\n"
      ],
      "metadata": {
        "id": "9CahZL3OoVXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    #return {\"answer\": response.content}\n",
        "    return {\"answer\": response}\n"
      ],
      "metadata": {
        "id": "9yQjk790oVaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "# Compile application and test\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "4YU98vslqw4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))\n",
        ""
      ],
      "metadata": {
        "id": "Z7q0J1onqw9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = graph.invoke({\"question\": \"what is langgraph?\"})\n",
        "print(response[\"answer\"])\n",
        ""
      ],
      "metadata": {
        "id": "qf9oVD-vqxBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Alw-j2a7qxJa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}